"""
Lab 03 - Sprint 1: Coleta de Dados de Pull Requests (Vers√£o Otimizada)
========================================================================
- Usa GraphQL para buscar reposit√≥rios
- Coleta at√© encontrar 200 reposit√≥rios V√ÅLIDOS (com >=100 PRs)
- Limita coleta a 500 PRs V√ÅLIDOS por reposit√≥rio
- Implementa pagina√ß√£o adequada

"""

import os
import sys
import json
import csv
import time
import asyncio
import threading
from datetime import datetime, timedelta
from typing import List, Dict, Optional
from concurrent.futures import ThreadPoolExecutor, as_completed
import requests
from dotenv import load_dotenv

# Configura encoding UTF-8 para Windows
if sys.platform == 'win32':
    import io
    sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8')
    sys.stderr = io.TextIOWrapper(sys.stderr.buffer, encoding='utf-8')

# For√ßa flush imediato dos prints
sys.stdout.reconfigure(line_buffering=True)

# Carrega vari√°veis de ambiente
load_dotenv('config.env')

# Configura√ß√µes
GITHUB_TOKEN = os.getenv('GITHUB_TOKEN')
MODE = os.getenv('MODE', 'test')

if MODE == 'test':
    MAX_REPOSITORIES = int(os.getenv('TEST_MAX_REPOSITORIES', 3))
    MAX_PRS_PER_REPO = int(os.getenv('TEST_MAX_PRS_PER_REPO', 5))
    MIN_PRS = int(os.getenv('TEST_MIN_PRS', 10))
else:
    MAX_REPOSITORIES = int(os.getenv('PROD_MAX_REPOSITORIES', 200))
    MAX_PRS_PER_REPO = int(os.getenv('PROD_MAX_PRS_PER_REPO', 500))
    MIN_PRS = int(os.getenv('PROD_MIN_PRS', 100))

DELAY_BETWEEN_REQUESTS = 1.0  # Cooldown de 1s entre requisi√ß√µes GraphQL
RATE_LIMIT_THRESHOLD = 500  # Pausa preventiva quando rate limit cai abaixo disso
RATE_LIMIT_PAUSE = 300  # Pausa de 5 minutos quando rate limit est√° baixo

# Arquivos de checkpoint
CHECKPOINT_FILE = f'data/checkpoint_{MODE}.json'
CHECKPOINT_CSV = f'data/checkpoint_{MODE}.csv'

# Headers
HEADERS = {
    'Authorization': f'token {GITHUB_TOKEN}',
    'Accept': 'application/vnd.github.v3+json'
}

GRAPHQL_HEADERS = {
    'Authorization': f'bearer {GITHUB_TOKEN}',
    'Content-Type': 'application/json'
}

# Lock para prints thread-safe
print_lock = threading.Lock()

def safe_print(msg, end='\n', flush=True):
    """Print thread-safe"""
    with print_lock:
        print(msg, end=end, flush=flush)


class GitHubAPIError(Exception):
    """Exce√ß√£o customizada para erros da API do GitHub"""
    pass


def make_request(url: str, params: Optional[Dict] = None, max_retries: int = 3) -> Optional[Dict]:
    """
    Faz requisi√ß√£o √† API com monitoramento proativo de rate limit
    """
    time.sleep(DELAY_BETWEEN_REQUESTS)  # Cooldown entre requisi√ß√µes
    
    for attempt in range(max_retries):
        try:
            response = requests.get(url, headers=HEADERS, params=params, timeout=30)
            
            # Monitora rate limit PROATIVAMENTE
            rate_limit_remaining = response.headers.get('X-RateLimit-Remaining')
            rate_limit_reset = response.headers.get('X-RateLimit-Reset')
            
            if rate_limit_remaining:
                remaining = int(rate_limit_remaining)
                
                # PAUSA PREVENTIVA quando rate limit est√° baixo
                if remaining < RATE_LIMIT_THRESHOLD:
                    reset_time = int(rate_limit_reset) if rate_limit_reset else 0
                    wait_time = reset_time - time.time() if reset_time > time.time() else RATE_LIMIT_PAUSE
                    
                    safe_print(f"\n‚ö†Ô∏è  RATE LIMIT PREVENTIVO: {remaining} requisi√ß√µes restantes")
                    safe_print(f"   ‚è∏Ô∏è  Pausando por {wait_time/60:.1f} minutos para evitar bloqueio...")
                    time.sleep(wait_time + 10)
                    safe_print(f"   ‚ñ∂Ô∏è  Retomando coleta...")
                
                # Avisa periodicamente
                elif remaining % 500 == 0:
                    safe_print(f"   ‚ö° Rate limit: {remaining} requisi√ß√µes restantes")
            
            # Tratamento de rate limit (caso ainda atinja)
            if response.status_code == 403:
                reset_time = int(response.headers.get('X-RateLimit-Reset', 0))
                wait_time = reset_time - time.time()
                if wait_time > 0:
                    safe_print(f"\n‚ùå RATE LIMIT ATINGIDO! Aguardando {wait_time/60:.1f} minutos...")
                    time.sleep(wait_time + 10)
                    continue
            
            # Tratamento de erros de servidor (502, 503, 504)
            if response.status_code in [502, 503, 504]:
                if attempt < max_retries - 1:
                    safe_print(f"\n‚ö†Ô∏è  Erro {response.status_code}, tentativa {attempt+1}/{max_retries}...")
                    time.sleep(15)
                    continue
                else:
                    safe_print(f"\n‚ùå Erro {response.status_code} persistente, pulando...")
                    return None
            
            if response.status_code == 200:
                return response.json()
            else:
                safe_print(f"\n‚ö†Ô∏è  Status {response.status_code}: {url}")
                return None
                
        except requests.exceptions.Timeout:
            if attempt < max_retries - 1:
                safe_print(f"\n‚ö†Ô∏è  Timeout, tentativa {attempt+1}/{max_retries}...")
                time.sleep(10)
                continue
            else:
                return None
        except Exception as e:
            safe_print(f"\n‚ö†Ô∏è  Erro: {e}")
            return None
    
    return None


def get_top_repositories_graphql(count: int = 200, min_prs: int = 100) -> List[Dict]:
    """
    Busca reposit√≥rios usando GraphQL at√© encontrar 'count' reposit√≥rios V√ÅLIDOS
    """
    print(f"\n{'='*70}", flush=True)
    print(f"üîç FASE 1: Buscando reposit√≥rios via GraphQL", flush=True)
    print(f"   Meta: {count} reposit√≥rios com pelo menos {min_prs} PRs", flush=True)
    print(f"{'='*70}", flush=True)
    
    url = "https://api.github.com/graphql"
    valid_repositories = []
    cursor = None
    total_searched = 0
    
    query = """
    query($cursor: String) {
      search(query: "stars:>1", type: REPOSITORY, first: 100, after: $cursor) {
        pageInfo {
          hasNextPage
          endCursor
        }
        nodes {
          ... on Repository {
            name
            nameWithOwner
            owner {
              login
            }
            stargazerCount
            forkCount
            primaryLanguage {
              name
            }
            createdAt
            url
            pullRequests(states: [MERGED, CLOSED]) {
              totalCount
            }
          }
        }
      }
    }
    """
    
    while len(valid_repositories) < count:
        variables = {"cursor": cursor}
        
        try:
            response = requests.post(
                url,
                json={"query": query, "variables": variables},
                headers=GRAPHQL_HEADERS,
                timeout=30
            )
            
            if response.status_code != 200:
                print(f"\n‚ùå Erro GraphQL: {response.status_code}", flush=True)
                break
            
            data = response.json()
            
            if 'errors' in data:
                print(f"\n‚ùå Erro GraphQL: {data['errors']}", flush=True)
                break
            
            search_result = data['data']['search']
            nodes = search_result['nodes']
            
            for repo in nodes:
                if repo:
                    total_searched += 1
                    total_prs = repo['pullRequests']['totalCount']
                    
                    if total_prs >= min_prs:
                        repo_info = {
                            'name': repo['name'],
                            'full_name': repo['nameWithOwner'],
                            'owner': repo['owner']['login'],
                            'stars': repo['stargazerCount'],
                            'forks': repo['forkCount'],
                            'language': repo['primaryLanguage']['name'] if repo['primaryLanguage'] else 'Unknown',
                            'created_at': repo['createdAt'],
                            'url': repo['url'],
                            'total_prs': total_prs
                        }
                        valid_repositories.append(repo_info)
                        
                        if len(valid_repositories) % 10 == 0:
                            print(f"   ‚úÖ {len(valid_repositories)}/{count} reposit√≥rios v√°lidos (de {total_searched} analisados)", flush=True)
                        
                        if len(valid_repositories) >= count:
                            break
            
            if len(valid_repositories) >= count:
                print(f"\n   üéØ Meta atingida! {count} reposit√≥rios v√°lidos coletados!", flush=True)
                break
            
            if not search_result['pageInfo']['hasNextPage']:
                print(f"\n   ‚ö†Ô∏è  Fim dos resultados. {len(valid_repositories)} reposit√≥rios v√°lidos encontrados.", flush=True)
                break
            
            cursor = search_result['pageInfo']['endCursor']
            time.sleep(1)  # Pausa entre p√°ginas GraphQL
            
        except Exception as e:
            print(f"\n‚ùå Erro na busca GraphQL: {e}", flush=True)
            break
    
    print(f"\n   üìä Total: {len(valid_repositories)} reposit√≥rios v√°lidos de {total_searched} analisados", flush=True)
    return valid_repositories


def get_prs_graphql(owner: str, repo: str, cursor: Optional[str] = None, max_prs: int = 100, max_retries: int = 3) -> tuple[List[Dict], Optional[str], bool]:
    """
    Busca PRs usando GraphQL - MUITO mais eficiente!
    Retorna (prs, next_cursor, has_next_page)
    Inclui retry logic para erros tempor√°rios (502, 503, timeout)
    """
    query = """
    query($owner: String!, $repo: String!, $cursor: String) {
      repository(owner: $owner, name: $repo) {
        pullRequests(first: 100, after: $cursor, states: [MERGED, CLOSED], orderBy: {field: UPDATED_AT, direction: DESC}) {
          pageInfo {
            hasNextPage
            endCursor
          }
          nodes {
            number
            title
            state
            createdAt
            closedAt
            mergedAt
            additions
            deletions
            changedFiles
            commits {
              totalCount
            }
            reviews {
              totalCount
            }
            comments {
              totalCount
            }
            author {
              login
            }
          }
        }
      }
    }
    """
    
    variables = {
        "owner": owner,
        "repo": repo,
        "cursor": cursor
    }
    
    for attempt in range(max_retries):
        try:
            time.sleep(DELAY_BETWEEN_REQUESTS)
            response = requests.post(
                "https://api.github.com/graphql",
                json={"query": query, "variables": variables},
                headers=GRAPHQL_HEADERS,
                timeout=30
            )
            
            # Erros tempor√°rios que podem ser retentados
            if response.status_code in [502, 503, 504]:
                retry_delay = 5 * (attempt + 1)  # 5s, 10s, 15s
                safe_print(f"\n‚ö†Ô∏è  GraphQL erro {response.status_code} (tentativa {attempt + 1}/{max_retries})")
                if attempt < max_retries - 1:
                    safe_print(f"   ‚è≥ Aguardando {retry_delay}s antes de tentar novamente...")
                    time.sleep(retry_delay)
                    continue
                else:
                    safe_print(f"   ‚ùå Todas as tentativas falharam")
                    return [], None, False
            
            if response.status_code != 200:
                safe_print(f"\n‚ö†Ô∏è  GraphQL erro {response.status_code}")
                return [], None, False
            
            data = response.json()
            
            if 'errors' in data:
                safe_print(f"\n‚ö†Ô∏è  GraphQL errors: {data['errors']}")
                return [], None, False
            
            pr_data = data['data']['repository']['pullRequests']
            page_info = pr_data['pageInfo']
            prs = pr_data['nodes']
            
            return prs, page_info['endCursor'], page_info['hasNextPage']
            
        except requests.exceptions.Timeout:
            retry_delay = 5 * (attempt + 1)
            safe_print(f"\n‚ö†Ô∏è  Timeout (tentativa {attempt + 1}/{max_retries})")
            if attempt < max_retries - 1:
                safe_print(f"   ‚è≥ Aguardando {retry_delay}s antes de tentar novamente...")
                time.sleep(retry_delay)
                continue
            else:
                safe_print(f"   ‚ùå Todas as tentativas falharam")
                return [], None, False
                
        except Exception as e:
            safe_print(f"\n‚ö†Ô∏è  Erro GraphQL: {e}")
            return [], None, False
    
    return [], None, False


def calculate_pr_metrics_from_graphql(pr: Dict, repo_full_name: str) -> Optional[Dict]:
    """Calcula m√©tricas de um PR vindo do GraphQL"""
    try:
        created_at = datetime.strptime(pr['createdAt'], '%Y-%m-%dT%H:%M:%SZ')
        
        if pr['closedAt']:
            closed_at = datetime.strptime(pr['closedAt'], '%Y-%m-%dT%H:%M:%SZ')
        else:
            return None
        
        analysis_time = closed_at - created_at
        analysis_hours = analysis_time.total_seconds() / 3600
        
        # Filtros:
        # 1. Tempo de an√°lise > 1 hora
        if analysis_hours <= 1.0:
            return None
        
        # 2. Precisa ter pelo menos 1 review
        num_reviews = pr['reviews']['totalCount']
        if num_reviews == 0:
            return None
        
        return {
            'repository': repo_full_name,
            'pr_number': pr['number'],
            'title': pr['title'],
            'status': 'MERGED' if pr['mergedAt'] else 'CLOSED',
            'created_at': pr['createdAt'],
            'closed_at': pr['closedAt'],
            'merged_at': pr.get('mergedAt'),
            'files_changed': pr.get('changedFiles', 0),
            'additions': pr.get('additions', 0),
            'deletions': pr.get('deletions', 0),
            'total_changes': pr.get('additions', 0) + pr.get('deletions', 0),
            'num_commits': pr['commits']['totalCount'],
            'num_reviews': num_reviews,
            'num_comments': pr['comments']['totalCount'],
            'analysis_time_hours': analysis_hours,
            'author': pr['author']['login'] if pr['author'] else 'unknown'
        }
    except Exception as e:
        return None


def collect_repository_prs(repo: Dict, max_prs: int) -> List[Dict]:
    """
    Coleta at√© 'max_prs' PRs V√ÅLIDOS de um reposit√≥rio usando GraphQL
    - UMA requisi√ß√£o GraphQL busca 100 PRs com TODOS os dados (reviews, comments, etc)
    - Muito mais eficiente que REST API (que precisava de 3-4 requests por PR)
    """
    owner = repo['owner']
    name = repo['name']
    full_name = repo['full_name']
    
    safe_print(f"\n{'‚îÄ'*70}")
    safe_print(f"üìä Reposit√≥rio: {full_name}")
    safe_print(f"   ‚≠ê Stars: {repo['stars']:,} | Total PRs: {repo['total_prs']}")
    
    valid_prs = []
    cursor = None
    page = 1
    total_processed = 0
    had_error = False
    
    # Continua at√© encontrar max_prs PRs V√ÅLIDOS ou acabar os PRs
    while len(valid_prs) < max_prs:
        # GraphQL: 1 requisi√ß√£o busca 100 PRs com TODAS as informa√ß√µes
        safe_print(f"   üì¶ P√°gina {page}: Buscando 100 PRs via GraphQL...")
        prs_batch, next_cursor, has_next = get_prs_graphql(owner, name, cursor, 100)
        
        if not prs_batch:
            if page == 1:
                safe_print(f"   ‚ö†Ô∏è  Erro ao buscar PRs (poss√≠vel erro 502/503 do GitHub)")
                had_error = True
            else:
                safe_print(f"   ‚ÑπÔ∏è  Nenhum PR retornado nesta p√°gina")
            break
        
        safe_print(f"   ‚úÖ Recebidos {len(prs_batch)} PRs (1 requisi√ß√£o GraphQL!)")
        
        # Processa PRs do batch
        page_results = []
        for pr in prs_batch:
            total_processed += 1
            
            # Calcula m√©tricas (j√° temos todos os dados!)
            metrics = calculate_pr_metrics_from_graphql(pr, full_name)
            
            if metrics:
                page_results.append(metrics)
                safe_print(f"      ‚úÖ PR #{pr['number']} v√°lido")
            else:
                safe_print(f"      ‚ùå PR #{pr['number']} filtrado")
        
        # Adiciona PRs v√°lidos da p√°gina
        valid_prs.extend(page_results)
        safe_print(f"   üìä P√°gina {page} conclu√≠da: {len(page_results)} v√°lidos | Total acumulado: {len(valid_prs)}/{max_prs}")
        
        # Verifica se deve continuar
        if len(valid_prs) >= max_prs:
            valid_prs = valid_prs[:max_prs]
            safe_print(f"   üéØ Meta de {max_prs} PRs v√°lidos alcan√ßada!")
            break
        
        if not has_next:
            safe_print(f"   ‚ÑπÔ∏è  √öltima p√°gina alcan√ßada")
            break
        
        cursor = next_cursor
        page += 1
    
    safe_print(f"\n   üìä RESULTADO: {len(valid_prs)} PRs v√°lidos de {total_processed} processados")
    
    if len(valid_prs) == 0 and had_error:
        safe_print(f"   ‚ö†Ô∏è  ATEN√á√ÉO: 0 PRs coletados devido a erro de comunica√ß√£o com GitHub")
    elif len(valid_prs) == 0:
        safe_print(f"   ‚ÑπÔ∏è  Nenhum PR atendeu aos crit√©rios (>1h an√°lise + >=1 revis√£o)")
    
    return valid_prs


def load_checkpoint() -> tuple[List[Dict], set]:
    """
    Carrega checkpoint existente e retorna (all_prs, processed_repos)
    """
    if os.path.exists(CHECKPOINT_FILE):
        try:
            with open(CHECKPOINT_FILE, 'r', encoding='utf-8') as f:
                data = json.load(f)
                all_prs = data.get('prs', [])
                processed_repos = set(data.get('processed_repos', []))
                safe_print(f"\nüìÇ Checkpoint encontrado!")
                safe_print(f"   ‚úÖ {len(all_prs)} PRs j√° coletados")
                safe_print(f"   ‚úÖ {len(processed_repos)} reposit√≥rios j√° processados")
                return all_prs, processed_repos
        except Exception as e:
            safe_print(f"\n‚ö†Ô∏è  Erro ao carregar checkpoint: {e}")
    
    return [], set()


def save_checkpoint(all_prs: List[Dict], processed_repos: set):
    """
    Salva checkpoint incremental (atualiza o mesmo arquivo)
    """
    try:
        os.makedirs('data', exist_ok=True)
        
        # Salva JSON
        checkpoint_data = {
            'timestamp': datetime.now().isoformat(),
            'total_prs': len(all_prs),
            'total_repos': len(processed_repos),
            'processed_repos': list(processed_repos),
            'prs': all_prs
        }
        
        with open(CHECKPOINT_FILE, 'w', encoding='utf-8') as f:
            json.dump(checkpoint_data, f, indent=2, ensure_ascii=False)
        
        # Salva CSV
        if all_prs:
            keys = all_prs[0].keys()
            with open(CHECKPOINT_CSV, 'w', newline='', encoding='utf-8') as f:
                writer = csv.DictWriter(f, fieldnames=keys)
                writer.writeheader()
                writer.writerows(all_prs)
        
        safe_print(f"   üíæ Checkpoint salvo: {len(all_prs)} PRs, {len(processed_repos)} repos")
        
    except Exception as e:
        safe_print(f"   ‚ö†Ô∏è  Erro ao salvar checkpoint: {e}")


def save_data(prs: List[Dict], mode: str, suffix: str = ""):
    """Salva dados em JSON e CSV"""
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    
    # JSON
    json_file = f'data/lab03_{mode}{suffix}_{timestamp}.json'
    with open(json_file, 'w', encoding='utf-8') as f:
        json.dump(prs, f, indent=2, ensure_ascii=False)
    
    # CSV
    csv_file = f'data/lab03_{mode}{suffix}_{timestamp}.csv'
    if prs:
        keys = prs[0].keys()
        with open(csv_file, 'w', newline='', encoding='utf-8') as f:
            writer = csv.DictWriter(f, fieldnames=keys)
            writer.writeheader()
            writer.writerows(prs)
    
    return json_file, csv_file


def generate_statistics(prs: List[Dict]) -> Dict:
    """Gera estat√≠sticas dos PRs coletados"""
    if not prs:
        return {}
    
    merged = [pr for pr in prs if pr['status'] == 'MERGED']
    closed = [pr for pr in prs if pr['status'] == 'CLOSED']
    
    stats = {
        'total_prs': len(prs),
        'merged_prs': len(merged),
        'closed_prs': len(closed),
        'repositories_count': len(set(pr['repository'] for pr in prs)),
        'metrics': {
            'files_changed': {
                'mean': sum(pr['files_changed'] for pr in prs) / len(prs),
                'min': min(pr['files_changed'] for pr in prs),
                'max': max(pr['files_changed'] for pr in prs)
            },
            'total_changes': {
                'mean': sum(pr['total_changes'] for pr in prs) / len(prs),
                'min': min(pr['total_changes'] for pr in prs),
                'max': max(pr['total_changes'] for pr in prs)
            },
            'analysis_time_hours': {
                'mean': sum(pr['analysis_time_hours'] for pr in prs) / len(prs),
                'min': min(pr['analysis_time_hours'] for pr in prs),
                'max': max(pr['analysis_time_hours'] for pr in prs)
            },
            'num_reviews': {
                'mean': sum(pr['num_reviews'] for pr in prs) / len(prs),
                'min': min(pr['num_reviews'] for pr in prs),
                'max': max(pr['num_reviews'] for pr in prs)
            },
            'num_comments': {
                'mean': sum(pr['num_comments'] for pr in prs) / len(prs),
                'min': min(pr['num_comments'] for pr in prs),
                'max': max(pr['num_comments'] for pr in prs)
            }
        }
    }
    
    return stats


def main():
    """Fun√ß√£o principal"""
    print("="*70, flush=True)
    print("Lab 03 - Sprint 1: Coleta de Dados de Pull Requests", flush=True)
    print(f"Modo: {MODE.upper()}", flush=True)
    print("="*70, flush=True)
    
    os.makedirs('data', exist_ok=True)
    
    start_time = time.time()
    
    # Carrega checkpoint se existir
    all_prs, processed_repos = load_checkpoint()
    
    # FASE 1: Buscar reposit√≥rios V√ÅLIDOS
    repositories = get_top_repositories_graphql(MAX_REPOSITORIES, MIN_PRS)
    
    if not repositories:
        print("\n‚ùå Nenhum reposit√≥rio encontrado!", flush=True)
        return
    
    # Filtra reposit√≥rios j√° processados
    remaining_repos = [r for r in repositories if r['full_name'] not in processed_repos]
    
    if remaining_repos:
        safe_print(f"\nüìã {len(remaining_repos)} reposit√≥rios restantes para processar (de {len(repositories)} totais)")
    else:
        safe_print(f"\n‚úÖ Todos os {len(repositories)} reposit√≥rios j√° foram processados!")
        if all_prs:
            safe_print(f"   Total de PRs coletados: {len(all_prs)}")
        return
    
    # FASE 2: Coletar PRs
    print(f"\n{'='*70}", flush=True)
    print(f"üì• FASE 2: Coletando PRs dos reposit√≥rios", flush=True)
    print(f"   Limite: {MAX_PRS_PER_REPO} PRs v√°lidos por reposit√≥rio", flush=True)
    print(f"   üíæ Checkpoint incremental ativado", flush=True)
    print(f"{'='*70}", flush=True)
    
    for idx, repo in enumerate(remaining_repos, 1):
        repo_name = repo['full_name']
        total_idx = len(repositories) - len(remaining_repos) + idx
        
        print(f"\n[{total_idx}/{len(repositories)}] Coletando {repo_name}...", flush=True)
        
        try:
            repo_prs = collect_repository_prs(repo, MAX_PRS_PER_REPO)
            all_prs.extend(repo_prs)
            processed_repos.add(repo_name)
            
            print(f"   ‚úÖ {len(repo_prs)} PRs coletados | Total acumulado: {len(all_prs)}", flush=True)
            
            # Salva checkpoint incremental (mesmo arquivo)
            save_checkpoint(all_prs, processed_repos)
            
        except Exception as e:
            print(f"   ‚ùå Erro: {e}", flush=True)
            # Mesmo com erro, salva checkpoint
            save_checkpoint(all_prs, processed_repos)
            continue
    
    # Salvar dados finais
    if all_prs:
        print(f"\n{'='*70}", flush=True)
        print(f"üíæ Salvando dados finais...", flush=True)
        json_file, csv_file = save_data(all_prs, MODE, "_final")
        print(f"   JSON: {json_file}", flush=True)
        print(f"   CSV: {csv_file}", flush=True)
        
        # Estat√≠sticas
        stats = generate_statistics(all_prs)
        stats_file = f'data/lab03_{MODE}_stats_{datetime.now().strftime("%Y%m%d_%H%M%S")}.json'
        with open(stats_file, 'w', encoding='utf-8') as f:
            json.dump(stats, f, indent=2)
        
        print(f"\n{'='*70}", flush=True)
        print(f"üìä ESTAT√çSTICAS FINAIS", flush=True)
        print(f"{'='*70}", flush=True)
        print(f"‚úÖ Total de PRs coletados: {len(all_prs)}", flush=True)
        print(f"üì¶ Reposit√≥rios processados: {len(processed_repos)}", flush=True)
        print(f"‚úîÔ∏è  PRs Merged: {stats['merged_prs']}", flush=True)
        print(f"‚ùå PRs Closed: {stats['closed_prs']}", flush=True)
        print(f"‚è±Ô∏è  Tempo total: {(time.time() - start_time)/60:.1f} minutos", flush=True)
        print(f"{'='*70}", flush=True)
    else:
        print("\n‚ùå Nenhum PR v√°lido coletado!", flush=True)


if __name__ == '__main__':
    main()
